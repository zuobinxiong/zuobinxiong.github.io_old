<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home</title>
    <description>Hope is a good thing, maybe the best of things, and no good thing ever dies.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 24 Nov 2022 01:10:48 -0500</pubDate>
    <lastBuildDate>Thu, 24 Nov 2022 01:10:48 -0500</lastBuildDate>
    <generator>Jekyll v4.2.2</generator>
    
      <item>
        <title>Federated Generative Model on Multi-Source Heterogeneous Data in IoT</title>
        <description>&lt;p&gt;&lt;strong&gt;Published in AAAI Conference on Artificial Intelligence&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The study of generative models is a promising branch of deep learning techniques, which has been successfully applied to different scenarios, such as Artificial Intelligence and the Internet of Things. While in most of the existing works, the generative models are realized as a centralized structure, raising the threats of security and privacy and the overburden of
communication costs. Rare efforts have been committed to investigating distributed generative models, especially when the training data comes from multiple heterogeneous sources under realistic IoT settings. In this paper, to handle this challenging problem, we design a federated generative model framework that can learn a powerful generator for the hierarchical IoT systems. Particularly, our generative model framework can solve the problem of distributed data generation on multi-source heterogeneous data in two scenarios, i.e., feature related scenario and label related scenario. In addition, in our federated generative models, we develop a synchronous and an asynchronous updating methods to satisfy different application requirements. Extensive experiments on a simulated dataset and multiple real datasets are conducted to evaluate the data generation performance of our proposed generative models through comparison with the state-of-the-arts.&lt;/p&gt;
</description>
        <pubDate>Fri, 18 Nov 2022 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2022/11/18/FedGAN/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/11/18/FedGAN/</guid>
        
        <category>Generative Adversarial Networks</category>
        
        <category>Federated Learning</category>
        
        <category>Privacy Protection</category>
        
        
      </item>
    
      <item>
        <title>Towards Neural Network-based Communication System - Attack and Defense</title>
        <description>&lt;p&gt;&lt;strong&gt;Published in IEEE Transactions on Dependable and Secure Computing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Recent progress has witnessed the excellent success of neural networks in many emerging applications, such as image recognition, text classification, and speech analysis. In order to achieve secure communication, the utilization of neural networks has been realized yet has not raised sufficient research attention. Additionally, the existing neural network-based communication system falls short due to its critical security flaws. In this paper, we investigate the security vulnerabilities of the existing neural communication system. Based on our analysis, we design two kinds of attack models, including target man-in-the-middle attack and target fraud attack . After that, to improve the security performance of neural communication systems, we develop a new defense mechanism to facilitate two-way secure communication by separating secret key from plaintext and incorporating defensive loss into the training process. Moreover, we show the effectiveness of our proposed neural communication system via theoretical proof. Finally, we implement comprehensive real data experiments to evaluate the performance of our attack and defense methods from the aspects of classification accuracy, communication efficiency and communication qualify, which confirms the advantages of our proposed neural communication system compared with the state-of-the-art.&lt;/p&gt;
</description>
        <pubDate>Sat, 27 Aug 2022 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/08/27/SecureANC/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/08/27/SecureANC/</guid>
        
        <category>Privacy Protection</category>
        
        <category>Adversarial Attacks</category>
        
        
      </item>
    
      <item>
        <title>Privacy threat and defense for federated learning with non-iid data in AIoT</title>
        <description>&lt;p&gt;&lt;strong&gt;Published in IEEE Transactions on Industrial Informatics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Under the needs of processing huge amounts of data, providing high-quality service, and protecting user privacy in artificial intelligence of things (AIoT), federated learning (FL) has been treated as a promising technique to facilitate distributed learning with privacy protection. Although the importance of developing privacy-preserving FL has attracted a lot of attentions, the existing research only focuses on FL with independent identically distributed (i.i.d.) data and lacks study of non-i.i.d. scenario. What is worse, the assumption of i.i.d. data is impractical, reducing the performance of privacy protection in real applications. In this article, we carry out an innovative exploration of privacy protection in FL with non-i.i.d. data. First, a thorough analysis on privacy leakage in FL is conducted with proving the performance upper bound of privacy inference attack. Based on our analysis, a novel algorithm, 2DP-FL, is designed to achieve differential privacy by adding noise during training local models and when distributing global model. Especially, our 2DP-FL algorithm has a flexibility of noise addition to meet various needs and has a convergence upper bound. Finally, the real-data experiments can validate the results of our the oretical analysis and the advantages of 2DP-FL in privacy protection, learning convergence, and model accuracy.&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Apr 2021 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2021/04/19/FL-threat/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/04/19/FL-threat/</guid>
        
        <category>Federated Learning</category>
        
        <category>Privacy Protection</category>
        
        <category>Differential Privacy</category>
        
        
      </item>
    
      <item>
        <title>Generative Adversarial Networks - A Survey Towards Private and Secure Applications</title>
        <description>&lt;p&gt;&lt;strong&gt;Published in ACM Computing Surveys&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Generative Adversarial Networks (GANs) have promoted a variety of applications in computer vision and natural language processing, among others, due to its generative model’s compelling ability to generate realistic examples plausibly drawn from an existing distribution of samples. GAN not only provides impressive performance on data generation-based tasks but also stimulates fertilization for privacy and security oriented research because of its game theoretic optimization strategy. Unfortunately, there are no comprehensive surveys on GAN in privacy and security, which motivates this survey to summarize systematically. The existing works are classified into proper categories based on privacy and security functions, and this survey conducts a comprehensive analysis of their advantages and drawbacks. Considering that GAN in privacy and security is still at a very initial stage and has imposed unique challenges that are yet to be well addressed, this article also sheds light on some potential privacy and security applications with GAN and elaborates on some future research directions.&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Apr 2021 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2021/04/02/GAN-survey/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/04/02/GAN-survey/</guid>
        
        <category>Generative Adversarial Networks</category>
        
        <category>Privacy Protection</category>
        
        <category>Adversarial Attacks</category>
        
        
      </item>
    
      <item>
        <title>Multi-Source Adversarial Sample Attack on Autonomous Vehicles</title>
        <description>&lt;p&gt;&lt;strong&gt;Published in IEEE Transactions on Vehicular Technology&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Deep learning has an impressive performance of object detection and classification for autonomous vehicles. Nevertheless, the essential vulnerability of deep learning models to adversarial samples makes the autonomous vehicles suffer severe security and safety issues. Although a number of works have been proposed to study adversarial samples, only a few of them are designated for the scenario of autonomous vehicles. Moreover, the state-of-the-art attack models only focus on a single data source without considering the correlation among multiple data sources. To fill this blank, we propose two multi-source adversarial sample attack models, including the parallel attack model and the fusion attack model to simultaneously attack the image and LiDAR perception systems in the autonomous vehicles. In the parallel attack model, adversarial samples are generated from the original image and LiDAR data separately. In the fusion attack model, the adversarial samples of image and LiDAR can be generated from a low-dimension vector at the same time by fully exploring data correlation for data fusion and adversarial sample generation. Through comprehensive real-data experiments, we validate that our proposed models are more powerful and efficient to break down the perception systems of autonomous vehicles compared with the state-of-the-art. Furthermore, we simulate possible attack scenarios in Vehicular Ad hoc Networks (VANETs) to evaluate the attack performance of our proposed methods.&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Feb 2021 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2021/02/14/Multi-source/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/02/14/Multi-source/</guid>
        
        <category>Generative Adversarial Networks</category>
        
        <category>Privacy Protection</category>
        
        <category>Adversarial Attacks</category>
        
        
      </item>
    
      <item>
        <title>Adgan - protect your location privacy in camera data of auto-driving vehicles</title>
        <description>&lt;p&gt;&lt;strong&gt;Published in IEEE Transactions on Industrial Informatics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Computer vision and deep neural networks have been significantly promoting the development of visual perception in these years. Particularly, for autonomous vehicles, real-time image/video data is captured by onboard cameras and analyzed by computer vision techniques in many real applications. In the captured camera data, some contents can be used as auxiliary information to infer individuals’ locations and trajectories, which leads to severe privacy leakage but has been rarely studied. Thus, the goal of this article is to protect individuals’ location privacy by hiding side-channel information in the captured data while preserving the data utility for downstream applications. To this end, the technology of generative adversarial networks (GAN) is utilized to design two novel models, named ADGAN-I and ADGAN-II, both of which can take the original camera data as inputs and generate privacy-preserving outputs according to predefined sensitive object class. Thus, the processed camera data can defend location inference attack from adversaries in offline applications. Moreover, in ADGAN-I and ADGAN-II, the tradeoff between location privacy and data utility can be effectively balanced. Finally, the results of extensive real-data experiments validate the superiority of our proposed models over the state of the arts in utility preservation and privacy protection for autonomous vehicles’ images and videos.&lt;/p&gt;
</description>
        <pubDate>Tue, 20 Oct 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/10/20/Adgan2/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/10/20/Adgan2/</guid>
        
        <category>Generative Adversarial Networks</category>
        
        <category>Privacy Protection</category>
        
        
      </item>
    
      <item>
        <title>Privacy-Preserving Auto-Driving - a GAN-based Approach to Protect Vehicular Camera Data</title>
        <description>&lt;p&gt;&lt;strong&gt;Published in IEEE International Conference on Data Mining&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The autonomous driving (auto-driving) technology has been promoted significantly by the rapid advances in computer vision and deep neural networks. Auto-driving vehicles, nowadays, are fully equipped with numerous sensors such as cameras, geo-sensors, and radar sensors, to capture real-time data inside the vehicles and outside surroundings. Meanwhile, the captured data contains lots of private information about vehicles, drivers and passengers and thus faces a high risk of privacy breaches. Especially, side-channel information can be mined from camera data to identify vehicles’ locations and even trajectories, raising serious privacy issues. Unfortunately, the issue, how to resist location-inference attack for camera data in auto-driving, has never been addressed in literature. In this paper, we intend to fill this blank by developing a GAN-based image-toimage translation method named Auto-Driving GAN (ADGAN). Through performance comparisons between ADGAN and the state-of-the-art, the superiority of ADGAN can be validated - offering an effective tradeoff between recognition utility and privacy protection for camera data.&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Nov 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/11/08/Adgan1/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/11/08/Adgan1/</guid>
        
        <category>Generative Adversarial Networks</category>
        
        <category>Privacy Protection</category>
        
        
      </item>
    
      <item>
        <title>Research on trajectory data releasing method via differential privacy based on spatial partition</title>
        <description>&lt;p&gt;&lt;strong&gt;Published in Security and Communication Networks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A number of security and privacy challenges of cyber system are arising due to the rapidly evolving scale and complexity of modern system and networks. The cyber system is a fundamental ingredient for Internet of Things (IoT) and smart city which are driven by huge amount of data. These data carry a lot of information for mining and analysis, especially trajectory data. If unprotected trajectory data is released, it may disclose user’s personal privacy, such as home, religion, and behavior mode, which will endanger their personal security. Until now, many methods for protecting trajectory information have been proposed. However, these methods have the following deficiencies: (i) they cannot defend against speculative attacks if the attacker’s background knowledge is maximized; (ii) when studying the problem, they made some strong assumptions that did not match the reality; (iii) the implementation algorithm is complicated and the time complexity is high, which means that data cannot be executed quickly when the amount is large. So, in this paper, we propose a spatial partition based method to publish trajectory data via differential privacy. First, by exponential mechanism, we divide location set at the same time into different partitions fast and accurately. Then we propose another effective method to release trajectory in a differential private manner. We design experiment based on the real-life dataset and compare it with existing method. The results show that the trajectory dataset released by our algorithm has better usability while ensuring privacy.&lt;/p&gt;
</description>
        <pubDate>Sun, 11 Nov 2018 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2018/11/11/Trajectory/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/11/11/Trajectory/</guid>
        
        <category>Differential Privacy</category>
        
        <category>Trajactory Data</category>
        
        
      </item>
    
  </channel>
</rss>
